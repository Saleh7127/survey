# Seed-OSS

Seed-OSS is an open-source large language model family developed by the ByteDance Seed Team for general-purpose text generation and reasoning. This repository provides the open-source implementation and tooling for running Seed-OSS models with an optimized Python-based inference stack.

It focuses on vLLM-powered inference with an OpenAI-compatible API surface, including configurable chat templates and built-in tool calling with automatic tool choice and a custom parser. The project is intended for developers familiar with Python and modern LLM inference frameworks who want to integrate Seed-OSS into applications, services, or research workflows.

## Table of Contents

- [Requirements / Prerequisites](#requirements--prerequisites)
- [Installation / Setup](#installation--setup)
- [Quick Start / Basic Usage](#quick-start--basic-usage)
- [Configuration](#configuration)
- [Demo / Examples](#demo--examples)
- [Advanced Usage / Commands](#advanced-usage--commands)
- [Features](#features)
- [License](#license)

## Requirements / Prerequisites

- Python 3.x environment (recommended for modern `transformers` / `accelerate` / `vllm` stacks).
- 64-bit Linux is recommended for best compatibility with vLLM and `flash_attn`.
- NVIDIA GPU with recent CUDA-capable driver, sufficient memory for large models (e.g., 36B parameters) and support for:
  - `flash_attn` (FlashAttention 2)
  - bfloat16 or float16 computation
- Ability to install and run the following Python libraries (exact versions are managed via `requirements.txt`):
  - `transformers` (>= 4.55.0)
  - `accelerate` (>= 0.34.2)
  - `flash_attn` (>= 2.6.3)
  - `vllm`
- Network and filesystem access suitable for:
  - Downloading or mounting Seed-OSS model weights
  - Running a local OpenAI-compatible HTTP API server
- Basic familiarity with Python CLIs and LLM inference tooling (e.g., vLLM, attention backends, tensor parallelism).

## Installation / Setup

1. Clone the repository and move into its directory:

```bash
git clone https://github.com/ByteDance-Seed/seed-oss.git
cd seed-oss
```

2. (Recommended) Create and activate a Python virtual environment:

```bash
python -m venv .venv
source .venv/bin/activate  # On Windows: .venv\Scripts\activate
```

3. Install Python dependencies from the provided requirements file:

```bash
pip install -r requirements.txt
```

This will install all required libraries, including `transformers`, `accelerate`, `flash_attn`, and `vllm`.

4. (Optional) Verify the installation by importing the main libraries in a Python shell:

```bash
python -c "import transformers, accelerate, vllm; print('OK')"
```

## Quick Start / Basic Usage

After installation and model download are complete, you can bring up a local OpenAI-compatible server with vLLM and send your first request.

1. Start the vLLM server (from the project root):

```bash
python3 -m vllm.entrypoints.openai.api_server \
    --host localhost \
    --port 4321 \
    --enable-auto-tool-choice \
    --tool-call-parser seed_oss \
    --trust-remote-code \
    --model ./Seed-OSS-36B-Instruct \
    --chat-template ./Seed-OSS-36B-Instruct/chat_template.jinja \
    --tensor-parallel-size 8 \
    --dtype bfloat16 \
    --served-model-name seed_oss
```

This exposes an OpenAI-compatible `/v1` endpoint at `http://localhost:4321`.

2. In a separate terminal, run a minimal client:

```python
from openai import OpenAI

# Point the OpenAI client at the local vLLM server
client = OpenAI(
    base_url="http://localhost:4321/v1",
    api_key="dummy",  # not used by the local server, but required by the SDK
)

thinking_budget = 2048  # adjust as needed
stream = False          # set True to use streaming
max_new_tokens = 256

response = client.chat.completions.create(
    model="seed_oss",
    stream=stream,
    messages=[{"role": "user", "content": "How to make pasta?"}],
    max_tokens=max_new_tokens,
    temperature=1.1,
    top_p=0.95,
    extra_body={
        "chat_template_kwargs": {
            "thinking_budget": thinking_budget,
        }
    },
)

print(response.choices[0].message.content)
```

Run this script with:

```bash
python your_script.py
```

You should see a natural-language answer generated by the Seed-OSS model.

## Configuration

The Seed-OSS inference stack is primarily configured through vLLM server flags and per-request generation parameters. The main options you are likely to customize are:

| Option | Where it is set | Purpose | Typical values / notes |
|--------|-----------------|---------|-------------------------|
| `--model` | vLLM server CLI | Filesystem path or HF repo ID for the Seed-OSS model weights. Controls which model is loaded by the vLLM OpenAI-compatible server. | Example: `--model ./Seed-OSS-36B-Instruct`. Point this to your own fine-tuned checkpoint to serve a custom model. |
| `--chat-template` | vLLM server CLI | Path to the Jinja chat template used to format prompts and tool calls. Determines how messages are serialized into the model’s input. | Example: `--chat-template ./Seed-OSS-36B-Instruct/chat_template.jinja`. Use a different template file to customize conversational formatting. |
| `--tensor-parallel-size` | vLLM server CLI | Degree of tensor parallelism across GPUs. Controls how many devices share the model weights and computation. | Example: `--tensor-parallel-size 8`. Set this to the number of GPUs you want to use for a single model replica. |
| `--dtype` | vLLM server CLI | Inference data type for model weights and activations. Impacts memory usage and performance. | Example: `--dtype bfloat16`. Use a lower-precision type (e.g., `bfloat16`, `float16`) for better throughput if your hardware supports it. |
| `--served-model-name` | vLLM server CLI | Logical model name exposed via the OpenAI-compatible API. This is the name you pass as `model` in client calls. | Example: `--served-model-name seed_oss`. In the client you then call `client.chat.completions.create(model="seed_oss", ...)`. Change this if you want to host multiple models behind the same server. |
| `thinking_budget` | Client request body (`extra_body.chat_template_kwargs`) | Per-request budget controlling how much “thinking” or intermediate reasoning the template allows before producing the final answer. Typically affects the number of intermediate tokens or steps the model can spend reasoning. | Example: `extra_body={ "chat_template_kwargs": { "thinking_budget": thinking_budget } }`. Tune this per request to trade off latency/cost versus reasoning depth. |

To customize behavior:

- Adjust `--model` and `--chat-template` when you switch to a different checkpoint or a modified prompt/tool-calling template.
- Scale `--tensor-parallel-size` and `--dtype` according to your GPU configuration and performance requirements.
- Use a descriptive `--served-model-name` and reference that same value in your client’s `model=` parameter.
- Pass `thinking_budget` via `extra_body.chat_template_kwargs` in each request to control reasoning depth dynamically (e.g., lower for fast interactive use, higher for complex tasks).

## Demo / Examples

The repository includes several ready-to-run demos and example snippets that show how to use Seed-OSS with an OpenAI-compatible API, tool calling, and extended generation controls like `thinking_budget`.

### 1. Start a vLLM OpenAI-compatible server

Use the provided vLLM entrypoint to serve the model with Seed-OSS–specific tooling support:


This launches an OpenAI-compatible HTTP endpoint with automatic tool choice and the Seed-OSS chat template.

### 2. Basic generation script

To run an end-to-end text generation example against a locally available checkpoint:

```bash
python inference/generate.py
```

This script provides a minimal, script-based entry point for running Seed-OSS without hosting an API server.

### 3. Tool-calling and `thinking_budget` example

The Python client-side example demonstrates:

- Calling the vLLM OpenAI-compatible server as `model="seed_oss"`.
- Supplying `messages` and `tools` for tool calling.
- Letting the model automatically choose tools via `tool_choice="auto"`.
- Passing a custom `thinking_budget` through `extra_body` to control internal reasoning.

```python
response = client.chat.completions.create(
    model="seed_oss",
    stream=stream,
    messages=demo_context["messages"],
    tools=demo_context["tools"],
    tool_choice="auto",
    max_tokens=max_new_tokens,
    temperature=1.1,
    top_p=0.95,
    extra_body={
        "chat_template_kwargs": {
            "thinking_budget": thinking_budget
        }
    }
)

parse_output(response, stream=stream, tool_functions=tool_functions)
```

The `parse_output` helper illustrates how to consume streamed or non-streamed responses and dispatch tool calls to your own `tool_functions`.

### 4. Example contexts and tools

The repository also includes example `demo_context` definitions (messages plus tool schemas such as a `get_weather` function). These serve as templates for building your own tool-calling workflows: define tools in OpenAI function-call format, start the vLLM server with `--enable-auto-tool-choice --tool-call-parser seed_oss`, and reuse the client snippet above to drive and parse calls.

## Advanced Usage / Commands

The following patterns build on the basic serving and generation flows and are intended for more advanced integrations and deployments.

### Serving with `inference/vllm_serve.sh`

The repository includes a helper script to start a vLLM server with Seed-OSS and the recommended flags:

```bash
bash inference/vllm_serve.sh
```

Typical responsibilities of this script include:

- Launching `python3 -m vllm.entrypoints.openai.api_server`
- Binding to a configurable host/port
- Enabling Seed-OSS–specific options such as:
  - `--enable-auto-tool-choice`
  - `--tool-call-parser seed_oss`
  - `--trust-remote-code`
  - `--model` and `--chat-template` pointing to your Seed-OSS weights and template
  - `--tensor-parallel-size` and `--dtype` for performance tuning
  - `--served-model-name` (e.g., `seed_oss`)

Use this script as a reference for your own deployment wrappers (systemd, Docker, Kubernetes) by copying and adjusting its arguments to match your environment (paths, ports, GPU count, etc.).

### OpenAI-compatible client with tools and auto tool choice

With the server running, you can use the standard `openai` Python client (configured to talk to vLLM) and take advantage of Seed-OSS’s tool-calling capabilities:

```python
from openai import OpenAI
from vllm_output_parser import parse_output

client = OpenAI(base_url="http://localhost:4321/v1", api_key="dummy")

demo_context = {
    "messages": [
        {"role": "user", "content": "You are a test system."},
        {"role": "assistant", "content": "Hi there!"},
        {"role": "user", "content": "Let me know the weather in Barcelona Spain."},
    ],
    "tools": [{
        "type": "function",
        "function": {
            "name": "get_weather",
            "description": "Get current temperature for a given location.",
            "parameters": {
                "type": "object",
                "properties": {
                    "location": {
                        "type": "string",
                        "description": "City and country e.g. Bogotá, Colombia"
                    },
                    "unit": {
                        "type": "string",
                        "description": "Temperature unit, e.g. celsius or fahrenheit"
                    }
                },
                "required": ["location"],
                "additionalProperties": False
            },
            "strict": True
        }
    }],
    "add_generation_prompt": True
}

thinking_budget = 2048
max_new_tokens = 4096
stream = True

response = client.chat.completions.create(
    model="seed_oss",
    stream=stream,
    messages=demo_context["messages"],
    tools=demo_context["tools"],
    tool_choice="auto",
    max_tokens=max_new_tokens,
    temperature=1.1,
    top_p=0.95,
    extra_body={
        "chat_template_kwargs": {
            "thinking_budget": thinking_budget
        }
    }
)
```

Important integration details:

- `tool_choice="auto"` leverages `--enable-auto-tool-choice` on the server to let Seed-OSS decide whether to call tools.
- `tools=demo_context["tools"]` supplies JSON-schema function definitions that the Seed-OSS tool parser understands.
- `extra_body["chat_template_kwargs"]["thinking_budget"]` controls an additional model-specific generation parameter exposed via the chat template.

### Streaming responses and parsing tool outputs

Seed-OSS provides a helper `parse_output(...)` (in `vllm_output_parser`) to simplify handling streaming responses and tool calls:

```python
import json

def get_weather(location: str, unit: str = "celsius"):
    # Your actual tool implementation here
    return {"location": location, "unit": unit, "temperature": 22.5}

tool_functions = {
    "get_weather": get_weather,
}

chat_response, tool_call, result = parse_output(
    response,
    stream=stream,
    tool_functions=tool_functions,
)
```

Typical behavior:

- If the model emits regular text only, `parse_output` yields the assistant content and prints it.
- If the model emits a tool call:
  - For streaming responses, it reconstructs `tool_name` and `tool_args_str` incrementally.
  - For non-streaming responses, it reads `response.choices[0].message.tool_calls[0]`.
  - It then:
    - Prints the function name and arguments.
    - Invokes the corresponding Python callable from `tool_functions`.
    - Returns a tuple like `(chat_response, tool_call, result)` where:
      - `chat_response` is the natural language output (if any),
      - `tool_call` contains the parsed function name and arguments,
      - `result` is whatever your tool function returns.

This pattern lets you:

- Keep your business logic in regular Python functions.
- Use Seed-OSS + vLLM as an orchestration layer that decides when to call those functions.
- Support both streaming and non-streaming interaction modes through the same `parse_output` interface.

### Advanced generation control with `thinking_budget`

Seed-OSS exposes a model-specific `thinking_budget` parameter via chat template kwargs:

- Set via `extra_body={"chat_template_kwargs": {"thinking_budget": <int>}}`.
- Typical usage:

  ```python
  response = client.chat.completions.create(
      model="seed_oss",
      messages=demo_context["messages"],
      tools=demo_context["tools"],
      tool_choice="auto",
      max_tokens=max_new_tokens,
      extra_body={
          "chat_template_kwargs": {
              "thinking_budget": 1024
          }
      }
  )
  ```

Guidelines for power users:

- Use a higher `thinking_budget` for complex reasoning or multi-step tool orchestration.
- Use a lower value (or `-1` for default behavior, as in the example driver script) to reduce latency and cost for simpler interactions.
- Combine `thinking_budget` with `max_tokens`, `temperature`, and `top_p` for finer control over reasoning depth vs. runtime.

## Features

- Open-source Seed-OSS large language model family (36B Base and Instruct variants) for general-purpose text generation, reasoning, and long-context applications.
- Inference scripts and integrations for serving Seed-OSS via vLLM with an OpenAI-compatible API surface, enabling drop-in use with existing OpenAI client libraries.
- Built-in tool-calling support, including automatic tool choice and a custom `seed_oss` tool-call parser for structured function calling workflows.
- Configurable chat templates that control conversation formatting, with additional generation controls such as a configurable `thinking_budget` for reasoning-intensive tasks.
- Optimized inference stack leveraging `transformers`, `accelerate`, `flash_attn`, and `vllm` to achieve efficient, high-throughput serving of the Seed-OSS models.

## License

ByteDance-Seed__seed-oss is licensed under the Apache-2.0 License. See the [LICENSE](./LICENSE) file for the full license text and terms.